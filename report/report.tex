\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!60!black,
    citecolor=blue!60!black,
    urlcolor=blue!60!black
}

% Custom colors
\definecolor{win}{RGB}{46,204,113}
\definecolor{loss}{RGB}{231,76,60}
\definecolor{neutral}{RGB}{149,165,166}

\title{\textbf{Data-Driven Ensemble Selection for Multi-LLM Systems}\\[0.5em]
\large An Empirical Study of Model Census, Complementarity Analysis,\\and Consensus Mechanisms}
\author{Ensemble AI Project --- Issue \#114}
\date{February 2026}

\begin{document}
\maketitle

\begin{abstract}
We present an empirical study of multi-model LLM ensembles, investigating whether data-driven model selection can produce ensembles that reliably outperform the best individual model.
Through a four-phase experiment---model census (20 models, 3 datasets), strength-band analysis, candidate ensemble formation, and full benchmarking (n=40)---we observe that ensembles appear to beat the best individual on knowledge/reasoning tasks (TruthfulQA: 5/5 configurations, +2.5 to +7.5 percentage points) but fail on mathematical tasks (GSM8K: 0/5 wins).
\textbf{Important caveat}: at our sample sizes ($n=10$--$40$ per dataset), confidence intervals exceed $\pm$14pp, meaning none of these differences are statistically significant. All findings should be treated as directional hypotheses requiring validation at $n \geq 200$.
Oracle ceiling analysis suggests complementary information exists; the bottleneck appears to be extracting it through effective consensus.
\end{abstract}

% ================================================================
\section{Introduction}
\label{sec:intro}

Large Language Models from different providers exhibit systematically different error patterns on the same task.
The premise of LLM ensembles is that combining responses from multiple models can yield higher accuracy than any single model---analogous to ensemble methods in classical machine learning.

However, naive ensemble construction (selecting models without regard to their relative strengths) often \emph{degrades} performance: a strong model's correct answer can be overruled by a majority of weaker models.
This motivates a data-driven approach: \textbf{measure} each candidate model's accuracy, \textbf{analyze} error complementarity between model pairs, and \textbf{select} ensembles where members are quality-matched but error-diverse.

\subsection{Research Questions}

\begin{enumerate}
    \item Does quality-matched ensemble selection produce ensembles that beat the best individual model?
    \item Which consensus mechanism best extracts the complementary signal?
    \item How does ensemble performance compare to self-consistency (multiple runs of one model)?
    \item Does the ensemble's value depend on task type?
\end{enumerate}

% ================================================================
\section{Methods}
\label{sec:methods}

\subsection{Datasets}

We evaluate on three benchmark datasets spanning different task types:

\begin{table}[htbp]
\centering
\caption{Benchmark datasets used in this study.}
\label{tab:datasets}
\begin{tabular}{llrll}
\toprule
\textbf{Dataset} & \textbf{Domain} & \textbf{Size} & \textbf{Answer Type} & \textbf{Evaluator} \\
\midrule
GSM8K & Grade-school math & 1,319 & Numeric & NumericEvaluator \\
TruthfulQA & Knowledge \& reasoning & 817 & Multiple choice (A--D) & MCQEvaluator \\
GPQA & Graduate-level science & 198 & Multiple choice (A--D) & MCQEvaluator \\
\bottomrule
\end{tabular}
\end{table}

For each benchmark run, questions are sampled randomly from the dataset.
Census runs use $n=20$ questions per model per dataset; full benchmark runs use $n=40$.

\subsection{Models}

We census 20 models across four providers (Table~\ref{tab:models}), spanning cheap, mid-tier, and upper-tier offerings.

\begin{table}[htbp]
\centering
\caption{Models included in the census ($n=20$ per dataset).}
\label{tab:models}
\small
\begin{tabular}{llll}
\toprule
\textbf{Provider} & \textbf{Model} & \textbf{Tier} & \textbf{Avg Accuracy} \\
\midrule
OpenAI & gpt-5 & upper & 85.0\% \\
OpenAI & gpt-5-mini & mid & 78.3\% \\
OpenAI & gpt-5-nano & cheap & 80.0\% \\
OpenAI & gpt-4.1 & upper & 50.0\% \\
OpenAI & gpt-4.1-mini & mid & 76.7\% \\
OpenAI & gpt-4.1-nano & cheap & 66.7\% \\
OpenAI & gpt-4o-mini & cheap & 41.7\% \\
\midrule
Anthropic & claude-sonnet-4.5 & upper & 25.0\% \\
Anthropic & claude-haiku-4.5 & mid & 48.3\% \\
Anthropic & claude-3.5-haiku & cheap & 41.7\% \\
\midrule
Google & gemini-2.5-pro & upper & 70.0\% \\
Google & gemini-2.5-flash & mid & 66.7\% \\
Google & gemini-2.5-flash-lite & cheap & 71.7\% \\
Google & gemini-2.0-flash & cheap & 73.3\% \\
Google & gemini-2.0-flash-lite & cheap & 68.3\% \\
Google & gemini-3-flash-preview & mid & 58.3\% \\
\midrule
xAI & grok-4.1-fast-reasoning & mid & 81.7\% \\
xAI & grok-4.1-fast-non-reasoning & mid & 60.0\% \\
xAI & grok-3 & mid & 56.7\% \\
xAI & grok-3-mini & cheap & 58.3\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experimental Design}

The experiment proceeds in four phases:

\paragraph{Phase 1: Model Census ($n=20$)}
Each model answers 20 randomly-sampled questions per dataset.
We record per-question correctness vectors, enabling pairwise complementarity analysis.
Total: $20 \times 3 \times 20 = 1{,}200$ evaluations.

\paragraph{Phase 2: Strength-Band Analysis}
Models are grouped into accuracy bands based on average accuracy across datasets:
\begin{itemize}
    \item \textbf{Band A} ($\geq$75\%): 5 models (gpt-5, grok-4.1-reasoning, gpt-5-nano, gpt-5-mini, gpt-4.1-mini)
    \item \textbf{Band B} (55--75\%): 10 models
    \item \textbf{Band C} (35--55\%): 4 models
    \item \textbf{Band D} ($<$35\%): 1 model
\end{itemize}

For each model pair, we compute \textbf{error complementarity}: the fraction of questions where exactly one model is correct.
High complementarity indicates the models make different errors---desirable for ensembles.

\paragraph{Phase 3: Candidate Ensemble Formation}
Based on strength bands and complementarity, we form five candidate ensembles (Table~\ref{tab:ensembles}).

\begin{table}[htbp]
\centering
\caption{Candidate ensemble configurations selected for full benchmarking.}
\label{tab:ensembles}
\small
\begin{tabular}{lll}
\toprule
\textbf{Ensemble} & \textbf{Selection Rationale} & \textbf{Models} \\
\midrule
Band-A (3) & Top-3 Band A, highest complementarity & grok-4.1-reason., gpt-5-mini, gpt-4.1-mini \\
Band-A (4) & Top-4 Band A & + gpt-5-nano \\
OpenAI (3) & Same-provider diversity & gpt-5, gpt-5-nano, gpt-5-mini \\
Cross-Provider & Best per provider & gpt-5, claude-haiku-4.5, gemini-2.5-pro, grok-4.1 \\
Band-B (3) & Top-3 Band B, highest complementarity & gpt-4.1-nano, gemini-2.5-flash, grok-4.1-non-reason. \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Phase 4: Full Benchmarks ($n=40$)}
Each candidate ensemble is evaluated on all three datasets with $n=40$ questions.
We test four consensus strategies:

\begin{enumerate}
    \item \textbf{Standard}: An LLM synthesizes a unified answer from all model responses.
    \item \textbf{Majority}: LLM-assisted majority vote across responses.
    \item \textbf{ELO}: Pairwise comparison ranking of responses.
    \item \textbf{Mechanical Majority}: Extract individual answers programmatically, take the statistical mode. No LLM involvement.
\end{enumerate}

Additionally, we test \textbf{self-consistency}: running one model $K$ times ($K=3,5$) with majority vote, for four top models.

\subsection{Metrics}

\begin{itemize}
    \item \textbf{Individual accuracy}: Fraction of questions each model answers correctly.
    \item \textbf{Consensus accuracy}: Fraction of questions the consensus answer is correct.
    \item \textbf{Delta}: Consensus accuracy minus best individual accuracy (positive = ensemble wins).
    \item \textbf{Oracle ceiling}: Accuracy if the ensemble always selects the correct answer when \emph{any} member gets it right. This is the theoretical maximum.
\end{itemize}

% ================================================================
\section{Results}
\label{sec:results}

\subsection{Phase 1: Model Census}

Figure~\ref{fig:census} shows the census results across 20 models.
Performance varies dramatically by model and dataset.
Several models that excel on GSM8K (e.g., gemini-2.0-flash-lite at 95\%) perform poorly on TruthfulQA (45\%), confirming that single-dataset evaluation is insufficient.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/fig1_census_heatmap.pdf}
    \caption{Model census results: accuracy (\%) by dataset for 20 models ($n=20$). Models sorted by average accuracy. Colors indicate provider.}
    \label{fig:census}
\end{figure}

Figure~\ref{fig:bands} shows the strength-band ranking.
The top 5 models (Band A, $\geq$75\% average) are led by gpt-5 (85.0\%) and grok-4.1-fast-reasoning (81.7\%).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig2_strength_bands.pdf}
    \caption{Model ranking by average accuracy across three datasets. Dashed lines indicate band thresholds.}
    \label{fig:bands}
\end{figure}

\subsection{Phase 2: Error Complementarity}

Figure~\ref{fig:complementarity} shows the pairwise error complementarity matrix for Band-A models.
The highest complementarity pair is grok-4.1-fast-reasoning $\times$ gpt-4.1-mini (0.25), confirming that models from different providers tend to make different errors.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.55\textwidth]{figures/fig7_complementarity.pdf}
    \caption{Error complementarity between Band-A models (fraction of questions where exactly one model is correct, averaged across datasets). Higher values indicate more diverse error patterns.}
    \label{fig:complementarity}
\end{figure}

\subsection{Phase 4: Ensemble Benchmarks}

\subsubsection{Ensemble vs.\ Best Individual}

Figure~\ref{fig:deltas} presents the main result: the accuracy delta of the best consensus strategy relative to the best individual model in each ensemble.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig3_ensemble_deltas.pdf}
    \caption{Accuracy delta (pp) of best consensus strategy vs.\ best individual model. Positive values (green) indicate ensemble wins; negative (red) indicate losses.}
    \label{fig:deltas}
\end{figure}

\paragraph{TruthfulQA: 5/5 ensemble wins.}
Every ensemble configuration outperforms the best individual model:

\begin{table}[htbp]
\centering
\caption{TruthfulQA results: all ensembles beat the best individual model.}
\label{tab:truthfulqa}
\begin{tabular}{lrrr}
\toprule
\textbf{Ensemble} & \textbf{Best Indiv.} & \textbf{Best Consensus} & \textbf{$\Delta$} \\
\midrule
Band-B (3) & 77.5\% & 85.0\% & \textcolor{win}{\textbf{+7.5pp}} \\
Cross-Provider & 87.5\% & 92.5\% & \textcolor{win}{\textbf{+5.0pp}} \\
Band-A (3) & 85.0\% & 87.5\% & \textcolor{win}{\textbf{+2.5pp}} \\
Band-A (4) & 85.0\% & 87.5\% & \textcolor{win}{\textbf{+2.5pp}} \\
OpenAI (3) & 85.0\% & 87.5\% & \textcolor{win}{\textbf{+2.5pp}} \\
\bottomrule
\end{tabular}
\end{table}

The cross-provider ensemble achieved 92.5\% accuracy, matching the oracle ceiling---meaning the consensus mechanism perfectly identified the correct answer whenever any ensemble member got it right.

\paragraph{GSM8K: 0/5 ensemble wins.}
On mathematical tasks, all LLM-based consensus strategies catastrophically fail.
Standard consensus drops to 7.5--20\% accuracy (from 85--95\% individual), because the synthesized text response cannot be reliably parsed by the numeric answer extractor.

\begin{table}[htbp]
\centering
\caption{GSM8K results: consensus mechanisms fail on numeric tasks.}
\label{tab:gsm8k}
\begin{tabular}{lrrrr}
\toprule
\textbf{Ensemble} & \textbf{Best Indiv.} & \textbf{Standard} & \textbf{Majority} & \textbf{Oracle} \\
\midrule
OpenAI (3) & 95.0\% & 10.0\% & 95.0\% & 97.5\% \\
Cross-Provider & 85.0\% & 7.5\% & 85.0\% & 97.5\% \\
Band-B (3) & 92.5\% & 15.0\% & 80.0\% & 97.5\% \\
Band-A (3) & 87.5\% & 15.0\% & 27.5\% & 100.0\% \\
Band-A (4) & 92.5\% & 20.0\% & 32.5\% & 100.0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Strategy Comparison}

Figure~\ref{fig:strategies} compares all four consensus strategies side-by-side for GSM8K and TruthfulQA.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/fig4_strategy_comparison.pdf}
    \caption{Consensus strategy comparison across ensemble configurations. On GSM8K (left), standard and ELO consensus catastrophically fail; on TruthfulQA (right), all strategies achieve reasonable accuracy.}
    \label{fig:strategies}
\end{figure}

Key findings:
\begin{itemize}
    \item \textbf{Majority vote} is the most robust strategy, performing comparably to or better than standard and ELO across task types.
    \item \textbf{Standard and ELO} generate free-text responses that work well for MCQ tasks but fail on numeric extraction.
    \item \textbf{Mechanical majority} (no LLM) matches LLM-assisted consensus on TruthfulQA, suggesting the value comes from answer diversity rather than sophisticated reasoning.
\end{itemize}

\subsubsection{Oracle Ceiling Analysis}

Figure~\ref{fig:oracle} shows the gap between what the ensemble \emph{could} achieve (oracle ceiling) and what it actually achieves.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig5_oracle_ceiling.pdf}
    \caption{Oracle ceiling (grey) vs.\ best consensus (green) vs.\ best individual (blue). Stars indicate cases where consensus beats the individual. Large oracle--consensus gaps indicate untapped potential.}
    \label{fig:oracle}
\end{figure}

Oracle ceilings of 85--100\% across all configurations confirm that \textbf{complementary information exists} in every ensemble.
On TruthfulQA, the cross-provider ensemble perfectly exploits this (consensus = oracle = 92.5\%).
On GSM8K, oracle ceilings reach 97.5--100\% but consensus achieves only 7.5--95\%, representing a 2.5--92.5pp extraction gap.

\subsection{Self-Consistency Comparison}

Figure~\ref{fig:sc} shows self-consistency results (K identical runs of one model with majority vote).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/fig6_self_consistency.pdf}
    \caption{Self-consistency accuracy delta relative to single response. Green bars indicate improvement; red indicates degradation.}
    \label{fig:sc}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Self-consistency summary: average delta by dataset and K value.}
\label{tab:sc_summary}
\begin{tabular}{lrrr}
\toprule
 & \textbf{GSM8K} & \textbf{TruthfulQA} & \textbf{GPQA} \\
\midrule
K=3, avg $\Delta$ & $-5.6$pp & $+0.6$pp & $+2.5$pp \\
K=5, avg $\Delta$ & $-1.9$pp & $+5.0$pp & $\pm0.0$pp \\
Best case & $+5.0$pp & $+10.0$pp & $+7.5$pp \\
Worst case & $-10.0$pp & $0.0$pp & $-2.5$pp \\
\bottomrule
\end{tabular}
\end{table}

Self-consistency \textbf{hurts} GSM8K performance (first answer is usually correct for math) but provides modest gains on knowledge tasks.
The best self-consistency result (+10.0pp for gpt-5-nano K=5 on TruthfulQA) exceeds most ensemble gains, but is less consistent across models.

\subsection{Self-Consistency: Literature Review}
\label{sec:sc_literature}

Our empirical self-consistency results above are consistent with a substantial body of research establishing self-consistency as one of the most effective inference-time techniques for improving LLM accuracy.

\paragraph{The Foundational Result.}
Wang et al.\ (2023, ICLR) introduced self-consistency in ``Self-Consistency Improves Chain of Thought Reasoning in Language Models.''
The core idea: sample multiple diverse reasoning paths from a single model using temperature $> 0$, then select the most frequent answer via majority vote.
This marginalizes over reasoning chains, exploiting the fact that correct reasoning paths converge on the same answer while incorrect paths scatter across different wrong answers.

Results on PaLM-540B showed dramatic improvements over chain-of-thought (CoT) baselines:

\begin{table}[htbp]
\centering
\caption{Self-consistency gains over CoT baseline (Wang et al., PaLM-540B, 40 paths).}
\label{tab:sc_wang}
\begin{tabular}{lrrr}
\toprule
\textbf{Benchmark} & \textbf{CoT Baseline} & \textbf{+ Self-Consistency} & \textbf{Gain} \\
\midrule
GSM8K & 56.5\% & 74.4\% & +17.9pp \\
SVAMP & 79.0\% & 86.6\% & +7.6pp \\
AQuA & 35.8\% & 48.3\% & +12.5pp \\
StrategyQA & 75.3\% & 81.6\% & +6.3pp \\
ARC-Challenge & 85.2\% & 88.7\% & +3.5pp \\
\bottomrule
\end{tabular}
\end{table}

Gains scaled with model size: UL2-20B saw 3--6pp improvements, while PaLM-540B achieved 4--18pp gains.
The optimal temperature range was $T=0.5$--$0.7$ with top-$k=40$.

\paragraph{Diminishing Returns and Sample Efficiency.}
The original paper evaluated 1, 5, 10, 20, and 40 sampled paths.
Performance improves monotonically but with strongly diminishing returns: the largest jump is from 1 to 5 samples, with smaller incremental gains beyond 10--20.

Recent work has focused on achieving equivalent accuracy with fewer samples:

\begin{itemize}
    \item \textbf{Confidence-Informed Self-Consistency (CISC)} (Taubenfeld et al., ACL 2025 Findings): Uses the model's own confidence scores ($P(\text{True})$) for weighted majority voting. On 9 models across GSM8K, MATH, MMLU-Pro, and Big-Bench-Hard, CISC with 10 samples matched standard SC requiring 15--21 samples---a 34--53\% cost reduction.

    \item \textbf{Reasoning-Aware Self-Consistency (RASC)} (NAACL 2025): Assesses reasoning path quality (not just final answers) for early stopping. Reduces sample usage by $\sim$70\% while maintaining accuracy.

    \item \textbf{Difficulty-Adaptive Self-Consistency} (NAACL 2025): Adaptively allocates samples based on query difficulty, reducing budget by up to 6$\times$ with $<$0.1\% accuracy loss across 13 datasets.
\end{itemize}

\paragraph{Universal Self-Consistency (USC).}
Chen et al.\ (2023, Google DeepMind) extended self-consistency to free-form text tasks where exact answer matching is impossible.
USC concatenates all sampled responses and asks the LLM itself to select the most consistent answer---performing semantic comparison rather than string matching.
USC matched standard SC on math/code tasks and enabled self-consistency on summarization, open QA, and TruthfulQA (achieving the highest truthfulness and informativeness over baselines).

\paragraph{When Self-Consistency Fails.}
Self-consistency is not universally beneficial.
A 2025 study on reasoning faithfulness found that Claude Opus 4.5 showed an \textbf{inverse relationship}: accuracy \emph{dropped} 3.7pp with $K=5$ while faithfulness surged (+230\%), breaking 23\% of previously-correct easy problems through ``overthinking.''
Models near ceiling accuracy (e.g., DeepSeek-v3.2 at 98--99\% on GSM8K) see negligible gains.
The technique works best when models are good-but-not-perfect and when errors are stochastic rather than systematic.

\paragraph{Self-Consistency vs.\ Heterogeneous Ensembles.}
Wang et al.\ found that a typical model-ensemble approach (averaging predictions from multiple models) performs \emph{much worse} than self-consistency, because lower-capacity models drag down the stronger model's signal.
LLM-TOPLA (EMNLP 2024) confirmed this but showed that \textbf{optimized subsets} of 2--4 diverse models can outperform both naive ensembles and single-model SC.
The ICLR 2024 paper ``Large Language Models Cannot Self-Correct Reasoning Yet'' found that multi-agent debate provides \textbf{no additional benefit} over simple self-consistency when controlling for the same number of model calls.

\paragraph{Implications for Our System.}
Our empirical results (Table~\ref{tab:sc_summary}) showing SC hurts GSM8K but helps TruthfulQA are consistent with the literature: SC works best on knowledge/reasoning tasks with diverse answer spaces and is less effective on math where greedy decoding often finds the correct answer on the first try.
The key parameters validated by the literature---$T=0.5$--$0.7$, $K=5$ as the minimum useful ensemble, and majority vote as the aggregation method---align with our current implementation defaults ($T=0.7$, $K=5$, majority vote).
The research strongly supports continuing with self-consistency as a baseline, while pursuing heterogeneous ensembles with diverse providers for complementary error patterns.

% ================================================================
\section{Related Work and External Critique}
\label{sec:related}

Our findings align closely with recent work on LLM ensembles and independently-received critical feedback from domain experts.

\subsection{Mixture-of-Agents (MoA)}

The Mixture-of-Agents framework demonstrates that LLM ensembles can achieve state-of-the-art results on benchmarks like MMLU, MATH, and AlpacaEval.
However, a follow-up study (``Rethinking Mixture-of-Agents'', 2024) found that mixing different LLMs often lowers average quality compared to \textbf{Self-MoA}: querying the single best model multiple times with elevated temperature and running majority vote on the outputs---essentially self-consistency.
Our self-consistency results partially support this: SC occasionally exceeds ensemble gains (e.g., +10.0pp for gpt-5-nano K=5 on TruthfulQA).

\subsection{Known Failure Modes}

External critique identified four specific failure modes in naive LLM ensembles, all of which we observe in our data:

\paragraph{1.\ The Dilution Effect.}
When ensemble members are quality-mismatched, weaker models outvote the strongest.
Our cross-provider ensemble includes claude-haiku-4.5 (48.3\% avg) alongside gpt-5 (85.0\%), yet still achieves ensemble gains---suggesting that 4-member ensembles can tolerate one weak member if three strong members align.
However, Band-B ensembles with lower-accuracy members show smaller gains.

\paragraph{2.\ Correlated Errors.}
Frontier models trained on similar data make similar mistakes.
Our complementarity analysis (Figure~\ref{fig:complementarity}) confirms this: same-provider pairs show lower complementarity than cross-provider pairs, supporting the recommendation to use cross-provider ensembles.

\paragraph{3.\ LLM-as-Judge Bottleneck.}
Our standard and ELO consensus strategies use an LLM to synthesize or rank responses.
On TruthfulQA, these perform well (+5.0pp for standard consensus on cross-provider).
On GSM8K, they catastrophically fail---consistent with the documented verbosity bias where LLM judges prefer longer, more detailed answers even when a concise numeric answer is correct.

\paragraph{4.\ Vote Splitting (Parsing Fragmentation).}
Different models format the same correct answer differently (``42'', ``The answer is 42'', ``x = 42'').
Our mechanical majority vote implementation extracts the numeric value before voting, partially mitigating this.
However, the standard consensus strategy generates free text, reintroducing parsing fragility.

\subsection{Proposed Architectural Improvements}

Based on external feedback and our empirical results, we identify three high-priority improvements not yet implemented:

\begin{enumerate}
    \item \textbf{Proposer-Aggregator Architecture}: Send all model responses to the strongest model for critique rather than symmetric consensus. This lets the best model benefit from alternative perspectives without being outvoted.
    \item \textbf{Weighted Voting}: Weight each model's vote by its measured accuracy on similar questions. Our census data provides exactly the per-model accuracy needed for this.
    \item \textbf{Strict Output Schemas}: Force structured output (JSON/XML tags) from all models before consensus to eliminate parsing fragmentation entirely.
\end{enumerate}

% ================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Task Type Determines Ensemble Value}

The most striking finding is the complete divergence between TruthfulQA (100\% win rate) and GSM8K (0\% win rate).
This is not because math ensembles lack complementary signal---oracle ceilings reach 100\%---but because the consensus mechanism generates narrative text that the answer extractor cannot parse.

This suggests a clear engineering fix: \textbf{extract answers from individual responses before consensus}, rather than asking the LLM to synthesize a unified narrative.
A structured pipeline (extract $\rightarrow$ vote $\rightarrow$ return) would likely unlock the GSM8K oracle potential.

\subsection{The Consensus Bottleneck}

Our results identify the consensus mechanism as the primary bottleneck in ensemble performance.
Mechanical majority vote (no LLM involvement) matches LLM consensus on TruthfulQA, meaning the LLM adds no value beyond simple vote counting for MCQ tasks.
This has practical implications: mechanical voting is cheaper, faster, and more deterministic than LLM-based consensus.

\subsection{Provider Diversity Matters}

The cross-provider ensemble (one model per provider) achieved the highest absolute accuracy on TruthfulQA (92.5\%), matching the oracle ceiling.
This suggests that models from different providers have genuinely orthogonal error patterns, even when trained on similar data.
Cross-provider ensembles should be preferred over same-provider ensembles when possible.

\subsection{Ensemble Size: 3 Is Enough}

The 3-model ensembles performed as well as 4-model ensembles (Band-A (3) and Band-A (4) both achieved 87.5\% on TruthfulQA).
Adding a fourth model increases cost and latency without improving accuracy.
This is consistent with the diminishing-returns property of ensemble methods.

\subsection{Statistical Soundness}
\label{sec:stats}

\textbf{The results in this report are not statistically significant.}
We present them as directional observations, not as evidence of real effects.
All headline findings should be treated as hypotheses requiring validation at larger sample sizes.

\paragraph{Confidence Intervals.}
For a binomial proportion $\hat{p}$ with $n$ trials, the standard error is $\text{SE} = \sqrt{\hat{p}(1-\hat{p})/n}$ and the approximate 95\% confidence interval is $\hat{p} \pm 1.96 \cdot \text{SE}$.

\begin{table}[htbp]
\centering
\caption{95\% confidence interval widths at various sample sizes (assuming $p = 0.7$).}
\label{tab:ci_widths}
\begin{tabular}{rrrl}
\toprule
$n$ & SE & 95\% CI width & Interpretation \\
\midrule
10 & 14.5\% & $\pm$28.4pp & Cannot distinguish 42\% from 98\% \\
40 & 7.2\% & $\pm$14.2pp & Cannot detect $<$15pp differences \\
50 & 6.5\% & $\pm$12.7pp & Marginal; only large effects visible \\
100 & 4.6\% & $\pm$9.0pp & Minimum for meaningful comparison \\
500 & 2.0\% & $\pm$4.0pp & Good precision \\
\bottomrule
\end{tabular}
\end{table}

At our current sample sizes ($n=10$--$40$ per dataset), the confidence intervals are so wide that observed differences of 2--8 percentage points are \textbf{entirely consistent with random chance}.
The +7.5pp TruthfulQA gain (our strongest result) falls within the 95\% CI noise floor at $n=40$.

\paragraph{Required Sample Sizes.}
Using a two-proportion $z$-test for comparing single vs.\ ensemble accuracy (both at $\sim$70\%) with 80\% power at $\alpha = 0.05$:

\begin{itemize}
    \item To detect a \textbf{5pp} difference: $n \geq 783$ per condition
    \item To detect a \textbf{10pp} difference: $n \geq 199$ per condition
    \item To detect a \textbf{15pp} difference: $n \geq 90$ per condition
\end{itemize}

Even the most optimistic ensemble gain we report (+7.5pp) would require $n \approx 350$ per condition to confirm as real with statistical confidence.

\paragraph{Variance Sources.}
Multiple sources of variance compound the problem:
\begin{enumerate}
    \item \textbf{Question sampling}: Different random subsets yield different accuracy estimates. With $n=10$, a single hard or easy question shifts accuracy by 10pp.
    \item \textbf{Model non-determinism}: Even at temperature~0, API responses vary across calls due to batching, quantization, and infrastructure changes.
    \item \textbf{LLM judge variance}: The judge model's extraction/grading introduces additional noise, particularly for free-form answers.
    \item \textbf{Dataset bias}: TruthfulQA places the correct answer at index~0 for all 817 questions (ground truth always ``A''). Without choice shuffling, any positional bias inflates accuracy.
\end{enumerate}

\paragraph{Path Forward.}
To produce statistically meaningful results, future evaluations should:
\begin{enumerate}
    \item Increase sample size to $n \geq 200$ per dataset (detects 10pp effects).
    \item Report Wilson confidence intervals alongside point estimates.
    \item Use paired tests (McNemar's test) to compare single vs.\ ensemble on the same questions, which increases statistical power by controlling for question difficulty.
    \item Run multiple independent trials to estimate run-to-run variance.
    \item Shuffle answer choices for MCQ datasets to eliminate positional bias.
\end{enumerate}

\subsection{Other Limitations}

\begin{enumerate}
    \item \textbf{GPQA coverage}: Only 2/5 GPQA configurations completed cleanly due to API reliability issues. GPQA conclusions are tentative.
    \item \textbf{Consensus implementation}: Standard/ELO strategies generate free text. A structured extraction pipeline could change results significantly.
    \item \textbf{Homogeneous ensembles}: Current quick-eval uses 5 copies of the same model. The literature and our earlier heterogeneous results both suggest that cross-provider ensembles with diverse error patterns are necessary for meaningful gains.
\end{enumerate}

% ================================================================
\section{Recommendations}
\label{sec:recommendations}

For production ensemble systems:

\begin{enumerate}
    \item \textbf{Default to majority vote} for consensus---it is the most robust strategy across task types.
    \item \textbf{Use 3 models} as the default ensemble size---diminishing returns beyond 3.
    \item \textbf{Prefer cross-provider ensembles}---provider diversity provides genuine error diversity.
    \item \textbf{Add task-type detection}: For numeric tasks, extract answers before consensus rather than synthesizing free text.
    \item \textbf{Quality-match ensemble members}: Ensure models are within $\sim$10pp of each other in accuracy to prevent weak models from dragging down consensus.
\end{enumerate}

For future research:
\begin{enumerate}
    \item Increase sample size to $n \geq 200$ per dataset for statistically meaningful comparisons (see Section~\ref{sec:stats}).
    \item Implement structured answer extraction before consensus for numeric tasks.
    \item Test adaptive (embedding-based) per-question model selection.
    \item Investigate weighted voting based on measured per-model accuracy.
\end{enumerate}

% ================================================================
\section{Conclusion}
\label{sec:conclusion}

Our experiments suggest that data-driven ensemble selection \emph{may} produce LLM ensembles that outperform the best individual model on knowledge/reasoning tasks.
On TruthfulQA, all five tested configurations outperformed the best individual (up to +7.5pp), with the cross-provider ensemble achieving 92.5\%.

However, as discussed in Section~\ref{sec:stats}, \textbf{none of these results are statistically significant} at our current sample sizes.
The observed gains of 2.5--7.5pp fall well within the $\pm$14pp confidence interval at $n=40$.
The high variance in our results---particularly the complete divergence between task types---may reflect noise rather than genuine effects.

Two findings we have higher confidence in, because they are directional and consistent across configurations:
(1) majority vote is at least as good as LLM-synthesized consensus on MCQ tasks and avoids catastrophic failures on numeric tasks;
(2) oracle ceiling analysis confirms complementary signal exists between diverse models, even if our consensus mechanisms cannot yet reliably extract it.

The path to meaningful results requires larger sample sizes ($n \geq 200$), paired statistical tests, multiple independent trials, and heterogeneous cross-provider ensembles.

% ================================================================
\appendix
\section{Data Inventory}
\label{app:data}

\begin{table}[htbp]
\centering
\caption{Complete data inventory for this experiment.}
\begin{tabular}{llr}
\toprule
\textbf{Asset} & \textbf{Location} & \textbf{Count} \\
\midrule
Census results & \texttt{artifacts/eval/issue-114/census/} & 60 files \\
Ensemble benchmarks & \texttt{artifacts/eval/issue-114/ensembles/} & 15 files (12 clean) \\
Self-consistency & \texttt{artifacts/eval/issue-114/self-consistency/} & 22 files \\
Figure generation & \texttt{report/generate\_figures.py} & 7 figures \\
LaTeX source & \texttt{report/report.tex} & this document \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
